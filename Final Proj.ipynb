{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1c1c4-61c4-4154-89d0-c98a78de4ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135a17f-a095-42d4-9c4a-11c1ddef8d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b8a2f1-8833-43c8-a08b-91af01727af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a list of sentences (e.g., ['What is AI?', 'Tell me a joke']):  [         \"LaBSE\", \"paraphrase-multilingual-mpnet-base-v2\", \"paraphrase-multilingual-MiniLM-L12-v2\",         \"all-mpnet-base-v2\", \"all-MiniLM-L12-v2\", \"all-MiniLM-L6-v2\", \"static-similarity-mrl-multilingual-v1\",         \"sentence-t5-xl\", \"sentence-t5-xxl\", \"sentence-t5-large\", \"sentence-t5-base\",         \"static-retrieval-mrl-en-v1\", \"multi-qa-MiniLM-L6-cos-v1\", \"gtr-t5-xxl\", \"gtr-t5-xl\", \"gtr-t5-base\", \"gtr-t5-large\"     ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating model: all-MiniLM-L6-v2\n",
      "üìä all-MiniLM-L6-v2: Total PCA variance = 42.28%\n",
      "\n",
      "üîÑ Evaluating model: all-MiniLM-L12-v2\n",
      "üìä all-MiniLM-L12-v2: Total PCA variance = 41.34%\n",
      "\n",
      "üîÑ Evaluating model: all-mpnet-base-v2\n",
      "üìä all-mpnet-base-v2: Total PCA variance = 46.18%\n",
      "\n",
      "üîÑ Evaluating model: LaBSE\n",
      "üìä LaBSE: Total PCA variance = 47.44%\n",
      "\n",
      "üîÑ Evaluating model: multi-qa-MiniLM-L6-cos-v1\n",
      "üìä multi-qa-MiniLM-L6-cos-v1: Total PCA variance = 41.95%\n",
      "LaBSE\n",
      "\n",
      "üîÑ Evaluating model: all-MiniLM-L6-v2\n",
      "üìä all-MiniLM-L6-v2: Total PCA variance = 42.28%\n",
      "\n",
      "üîÑ Evaluating model: all-MiniLM-L12-v2\n",
      "üìä all-MiniLM-L12-v2: Total PCA variance = 41.34%\n",
      "\n",
      "üîÑ Evaluating model: all-mpnet-base-v2\n",
      "üìä all-mpnet-base-v2: Total PCA variance = 46.18%\n",
      "\n",
      "üîÑ Evaluating model: LaBSE\n",
      "üìä LaBSE: Total PCA variance = 47.44%\n",
      "\n",
      "üîÑ Evaluating model: multi-qa-MiniLM-L6-cos-v1\n",
      "üìä multi-qa-MiniLM-L6-cos-v1: Total PCA variance = 41.95%\n",
      "\n",
      "‚úÖ Best model based on PCA variance: LaBSE\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import gc\n",
    "\n",
    "# ‚úÖ Optimized model list (lightweight and performant)\n",
    "model_list = [\n",
    "    \"all-MiniLM-L6-v2\", \"all-MiniLM-L12-v2\", \"all-mpnet-base-v2\",\n",
    "    \"LaBSE\", \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "]\n",
    "\n",
    "# üì• Get user input\n",
    "user_input = input(\"Enter a list of sentences (e.g., ['What is AI?', 'Tell me a joke']): \")\n",
    "try:\n",
    "    answerlist = ast.literal_eval(user_input)\n",
    "    if not isinstance(answerlist, list) or not all(isinstance(x, str) for x in answerlist):\n",
    "        raise ValueError\n",
    "except Exception:\n",
    "    print(\"‚ùå Invalid input. Please enter a valid Python list of strings.\")\n",
    "    exit()\n",
    "\n",
    "# üîç Select best model by PCA variance\n",
    "def get_best_model_by_variance(answerlist, model_list):\n",
    "    best_model = None\n",
    "    best_variance = 0.0\n",
    "\n",
    "    for model_name in model_list:\n",
    "        try:\n",
    "            print(f\"\\nüîÑ Evaluating model: {model_name}\")\n",
    "            model = SentenceTransformer(model_name)\n",
    "            embeddings = model.encode(answerlist, normalize_embeddings=False)\n",
    "            pca = PCA(n_components=2)\n",
    "            reduced = pca.fit_transform(embeddings)\n",
    "            total_var = sum(pca.explained_variance_ratio_)\n",
    "            print(f\"üìä {model_name}: Total PCA variance = {total_var * 100:.2f}%\")\n",
    "\n",
    "            if total_var > best_variance:\n",
    "                best_variance = total_var\n",
    "                best_model = model_name\n",
    "\n",
    "            # üöÆ Cleanup\n",
    "            del model, embeddings, reduced, pca\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è {model_name}: Skipped due to error: {e}\")\n",
    "\n",
    "    return best_model\n",
    "    \n",
    "print((get_best_model_by_variance(answerlist, model_list)))\n",
    "# üöÄ Determine best model and load it\n",
    "best_model_name = get_best_model_by_variance(answerlist, model_list)\n",
    "print(f\"\\n‚úÖ Best model based on PCA variance: {best_model_name}\")\n",
    "model = SentenceTransformer(best_model_name)\n",
    "\n",
    "# ‚ö° Precompute embeddings\n",
    "real_embeddings = model.encode(answerlist, convert_to_tensor=True)\n",
    "\n",
    "# üîé Single query function\n",
    "def answer(query):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, real_embeddings)[0]\n",
    "    best_idx = torch.argmax(similarities).item()\n",
    "    best_score = similarities[best_idx].item()\n",
    "    best_match = answerlist[best_idx]\n",
    "    print(f\"‚úÖ Best Match: '{best_match}' (score: {best_score:.4f})\")\n",
    "    return best_match\n",
    "\n",
    "# üîÑ Batch query function\n",
    "def batch_answer(queries):\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "    query_embeddings = model.encode(queries, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(query_embeddings, real_embeddings)\n",
    "    results = []\n",
    "    for i in range(len(queries)):\n",
    "        sim_row = similarities[i]\n",
    "        best_idx = torch.argmax(sim_row).item()\n",
    "        best_match = answerlist[best_idx]\n",
    "        results.append(best_match)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61f4d83-a047-477f-83e0-92496f3c2d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
